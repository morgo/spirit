package move

import (
	"context"
	"database/sql"
	"errors"
	"fmt"
	"sync/atomic"
	"time"

	"github.com/block/spirit/pkg/checksum"
	"github.com/block/spirit/pkg/copier"
	"github.com/block/spirit/pkg/dbconn"
	"github.com/block/spirit/pkg/metrics"
	"github.com/block/spirit/pkg/migration"
	"github.com/block/spirit/pkg/repl"
	"github.com/block/spirit/pkg/table"
	"github.com/block/spirit/pkg/throttler"
	"github.com/go-sql-driver/mysql"
	"github.com/siddontang/loggers"
	"github.com/sirupsen/logrus"
)

var (
	sentinelCheckInterval = 1 * time.Second
	sentinelWaitLimit     = 48 * time.Hour
	sentinelTableName     = "_spirit_sentinel" // this is now a const.
)

type Runner struct {
	move         *Move
	source       *sql.DB
	sourceConfig *mysql.Config
	target       *sql.DB
	targetConfig *mysql.Config
	currentState migration.State // must use atomic to get/set

	sourceTables []*table.TableInfo

	replClient      *repl.Client
	copyChunker     table.Chunker
	checksumChunker table.Chunker
	copier          copier.Copier
	checker         *checksum.Checker

	// Track some key statistics.
	startTime             time.Time
	sentinelWaitStartTime time.Time

	cutoverFunc func(ctx context.Context) error

	logger     loggers.Advanced
	cancelFunc context.CancelFunc
	dbConfig   *dbconn.DBConfig
}

func NewRunner(m *Move) (*Runner, error) {
	r := &Runner{
		move:   m,
		logger: logrus.New(),
	}
	return r, nil
}

func (r *Runner) Close() error {
	return nil
}

// getSourceTables connects to r.source and fetches the list of tables
// to populate r.sourceTables.
func (r *Runner) getSourceTables(ctx context.Context) error {
	rows, err := r.source.Query("SHOW TABLES")
	if err != nil {
		return err
	}
	defer rows.Close()

	var tableName string
	for rows.Next() {
		if err := rows.Scan(&tableName); err != nil {
			return err
		}
		tableInfo := table.NewTableInfo(r.source, r.sourceConfig.DBName, tableName)
		if err := tableInfo.SetInfo(ctx); err != nil {
			return err
		}
		r.sourceTables = append(r.sourceTables, tableInfo)
	}
	return rows.Err()
}

// checkTargetEmpty checks that the target database is empty.
// If any tables exist it returns an error and the move fails.
func (r *Runner) checkTargetEmpty() error {
	rows, err := r.target.Query("SHOW TABLES")
	if err != nil {
		return err
	}
	defer rows.Close()
	if rows.Next() {
		return errors.New("target database is not empty")
	}
	if err := rows.Err(); err != nil {
		return err
	}
	return nil
}

// createTargetTables fetches the CREATE TABLE statement for each table
// in r.sourceTables and runs it on r.target. For now, we require that
// the target is identical to the source. In future, we may create
// the target with secondary indexes disabled, and re-add them after.
func (r *Runner) createTargetTables() error {
	for _, t := range r.sourceTables {
		var createStmt string
		row := r.source.QueryRow(fmt.Sprintf("SHOW CREATE TABLE `%s`", t.TableName))
		var tbl string
		if err := row.Scan(&tbl, &createStmt); err != nil {
			return err
		}
		// Execute the create statement on the target.
		if _, err := r.target.Exec(createStmt); err != nil {
			return err
		}
	}
	return nil
}

func (r *Runner) Run(ctx context.Context) error {
	ctx, r.cancelFunc = context.WithCancel(ctx)
	defer r.cancelFunc()
	r.startTime = time.Now()
	r.logger.Infof("Starting table move")

	var err error
	r.dbConfig = dbconn.NewDBConfig()
	r.logger.Warn("the move command is experimental and not yet safe for production use.")
	r.source, err = dbconn.New(r.move.SourceDSN, r.dbConfig)
	if err != nil {
		return err
	}
	defer r.source.Close()
	r.target, err = dbconn.New(r.move.TargetDSN, r.dbConfig)
	if err != nil {
		return err
	}
	defer r.target.Close()

	r.sourceConfig, err = mysql.ParseDSN(r.move.SourceDSN)
	if err != nil {
		return err
	}
	r.targetConfig, err = mysql.ParseDSN(r.move.TargetDSN)
	if err != nil {
		return err
	}

	// Parse the r.move.SourceDSN and fetch the table list.
	if err = r.getSourceTables(ctx); err != nil {
		return err
	}

	// Check target is empty.
	if err := r.checkTargetEmpty(); err != nil {
		return err
	}

	// For each table, fetch the CREATE TABLE statement from the source and run it on the target.
	if err := r.createTargetTables(); err != nil {
		return err
	}

	// Create a sentinel.
	if r.move.CreateSentinel {
		if err := r.createSentinelTable(ctx); err != nil {
			return err
		}
	}

	r.replClient = repl.NewClient(r.source, r.sourceConfig.Addr, r.sourceConfig.User, r.sourceConfig.Passwd, &repl.ClientConfig{
		Logger:                     r.logger,
		Concurrency:                r.move.Threads,
		TargetBatchTime:            r.move.TargetChunkTime,
		ServerID:                   repl.NewServerID(),
		UseExperimentalBufferedMap: true,
		WriteDB:                    r.target,
	})

	copyChunkers := make([]table.Chunker, 0, len(r.sourceTables))
	checksumChunkers := make([]table.Chunker, 0, len(r.sourceTables))

	// For each table create a chunker and add a subscription to the replication client.
	for _, src := range r.sourceTables {
		dest := table.NewTableInfo(r.target, r.targetConfig.DBName, src.TableName)
		if err := dest.SetInfo(ctx); err != nil {
			return err
		}
		copyChunker, err := table.NewChunker(src, dest, r.move.TargetChunkTime, r.logger)
		if err != nil {
			return err
		}
		if err := r.replClient.AddSubscription(src, dest, copyChunker.KeyAboveHighWatermark); err != nil {
			return err
		}
		checksumChunker, err := table.NewChunker(src, dest, r.move.TargetChunkTime, r.logger)
		if err != nil {
			return err
		}
		copyChunkers = append(copyChunkers, copyChunker)
		checksumChunkers = append(checksumChunkers, checksumChunker)
	}

	// Then create a multi chunker of all chunkers.
	r.copyChunker = table.NewMultiChunker(copyChunkers...)
	r.checksumChunker = table.NewMultiChunker(checksumChunkers...)

	// Then open the multi chunker.
	if err := r.copyChunker.Open(); err != nil {
		return err
	}
	defer r.copyChunker.Close()

	// Create a copier that reads from the multi chunker and writes to the target.
	r.copier, err = copier.NewCopier(r.source, r.copyChunker, &copier.CopierConfig{
		Concurrency:                   r.move.Threads,
		TargetChunkTime:               r.move.TargetChunkTime,
		Logger:                        r.logger,
		Throttler:                     &throttler.Noop{},
		MetricsSink:                   &metrics.NoopSink{},
		DBConfig:                      r.dbConfig,
		UseExperimentalBufferedCopier: true,
		WriteDB:                       r.target,
	})
	if err != nil {
		return err
	}

	// Start the replication client.
	if err := r.replClient.Run(ctx); err != nil {
		return err
	}

	// Run the copier.
	r.setCurrentState(migration.StateCopyRows)
	if err := r.copier.Run(ctx); err != nil {
		return err
	}

	// When the copier has finished, catch up the replication client
	// This is in a non-blocking way first.
	if err := r.replClient.Flush(ctx); err != nil {
		return err
	}

	r.logger.Infof("All tables copied successfully.")

	r.sentinelWaitStartTime = time.Now()
	r.setCurrentState(migration.StateWaitingOnSentinelTable)
	if err := r.waitOnSentinelTable(ctx); err != nil {
		return err
	}

	// Perform a checksum operation + ANALYZE TABLEs
	// To make sure they are all in a ready state.
	if err := r.prepareForCutover(ctx); err != nil {
		return err
	}

	// Create a cutover.
	r.setCurrentState(migration.StateCutOver)
	cutover, err := NewCutOver(r.source, r.sourceTables, r.cutoverFunc, r.replClient, r.dbConfig, r.logger)
	if err != nil {
		return err
	}
	if err = cutover.Run(ctx); err != nil {
		return err
	}
	r.logger.Infof("Move operation complete.")
	return nil
}

func (r *Runner) SetLogger(logger loggers.Advanced) {
	r.logger = logger
}

func (r *Runner) prepareForCutover(ctx context.Context) error {
	// Disable the periodic flush and flush all pending events.
	// We want it disabled for ANALYZE TABLE and acquiring a table lock
	// *but* it will be started again briefly inside of the checksum
	// runner to ensure that the lag does not grow too long.
	r.replClient.StopPeriodicFlush()
	r.setCurrentState(migration.StateApplyChangeset)
	if err := r.replClient.Flush(ctx); err != nil {
		return err
	}

	// Run ANALYZE TABLE to update the statistics on the new table.
	// This is required so on cutover plans don't go sideways, which
	// is at elevated risk because the batch loading can cause statistics
	// to be out of date.
	r.setCurrentState(migration.StateAnalyzeTable)
	r.logger.Infof("Running ANALYZE TABLE")
	for _, tbl := range r.sourceTables {
		if err := dbconn.Exec(ctx, r.target, "ANALYZE TABLE %n.%n", tbl.SchemaName, tbl.TableName); err != nil {
			return err
		}
	}

	r.setCurrentState(migration.StateChecksum)
	if err := r.checksumChunker.Open(); err != nil {
		return err
	}
	defer r.checksumChunker.Close()

	// Perform a checksum operation
	// TODO: we currently don't support repairing in move
	// But if we do, we need to modify the checker so it can re-run on failure.
	var err error
	r.checker, err = checksum.NewChecker(r.source, r.checksumChunker, r.replClient, &checksum.CheckerConfig{
		Concurrency:     r.move.Threads,
		TargetChunkTime: r.move.TargetChunkTime,
		DBConfig:        r.dbConfig,
		Logger:          r.logger,
		WriteDB:         r.target,
	})
	if err != nil {
		return err
	}
	return r.checker.Run(ctx)
}

func (r *Runner) SetCutover(cutover func(ctx context.Context) error) {
	r.cutoverFunc = cutover
}

func (r *Runner) GetProgress() migration.Progress {
	var summary string
	switch r.getCurrentState() { //nolint: exhaustive
	case migration.StateCopyRows:
		summary = fmt.Sprintf("%v %s ETA %v",
			r.copier.GetProgress(),
			r.getCurrentState().String(),
			r.copier.GetETA(),
		)
	case migration.StateWaitingOnSentinelTable:
		r.logger.Infof("migration status: state=%s sentinel-table=%s.%s total-time=%s sentinel-wait-time=%s sentinel-max-wait-time=%s",
			r.getCurrentState().String(),
			r.targetConfig.DBName,
			sentinelTableName,
			time.Since(r.startTime).Round(time.Second),
			time.Since(r.sentinelWaitStartTime).Round(time.Second),
			sentinelWaitLimit,
		)
	case migration.StateApplyChangeset, migration.StatePostChecksum:
		summary = fmt.Sprintf("Applying Changeset Deltas=%v", r.replClient.GetDeltaLen())
	case migration.StateChecksum:
		summary = fmt.Sprintf("%v %s",
			r.checker.GetProgress(),
			r.getCurrentState().String(),
		)
	}
	return migration.Progress{
		CurrentState: r.getCurrentState().String(),
		Summary:      summary,
	}
}

func (r *Runner) getCurrentState() migration.State {
	return migration.State(atomic.LoadInt32((*int32)(&r.currentState)))
}

func (r *Runner) setCurrentState(s migration.State) {
	atomic.StoreInt32((*int32)(&r.currentState), int32(s))
}

func (r *Runner) createSentinelTable(ctx context.Context) error {
	if err := dbconn.Exec(ctx, r.target, "DROP TABLE IF EXISTS %n.%n", r.targetConfig.DBName, sentinelTableName); err != nil {
		return err
	}
	if err := dbconn.Exec(ctx, r.target, "CREATE TABLE %n.%n (id int NOT NULL PRIMARY KEY)", r.targetConfig.DBName, sentinelTableName); err != nil {
		return err
	}
	return nil
}

func (r *Runner) sentinelTableExists(ctx context.Context) (bool, error) {
	sql := "SELECT COUNT(*) FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = ? AND TABLE_NAME = ?"
	var sentinelTableExists int
	err := r.target.QueryRowContext(ctx, sql, r.targetConfig.DBName, sentinelTableName).Scan(&sentinelTableExists)
	if err != nil {
		return false, err
	}
	return sentinelTableExists > 0, nil
}

// Check every sentinelCheckInterval up to sentinelWaitLimit to see if sentinelTable has been dropped
func (r *Runner) waitOnSentinelTable(ctx context.Context) error {
	if sentinelExists, err := r.sentinelTableExists(ctx); err != nil {
		return err
	} else if !sentinelExists {
		// Sentinel table does not exist, we can proceed with cutover
		return nil
	}

	r.logger.Warnf("cutover deferred while sentinel table %s exists; will wait %s", sentinelTableName, sentinelWaitLimit)

	timer := time.NewTimer(sentinelWaitLimit)
	defer timer.Stop() // Ensure timer is always stopped to prevent goroutine leak

	ticker := time.NewTicker(sentinelCheckInterval)
	defer ticker.Stop()
	for {
		select {
		case t := <-ticker.C:
			sentinelExists, err := r.sentinelTableExists(ctx)
			if err != nil {
				return err
			}
			if !sentinelExists {
				// Sentinel table has been dropped, we can proceed with cutover
				r.logger.Infof("sentinel table dropped at %s", t)
				return nil
			}
		case <-timer.C:
			return errors.New("timed out waiting for sentinel table to be dropped")
		}
	}
}
